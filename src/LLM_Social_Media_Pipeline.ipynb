{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27a7ce77-fc5f-463f-a403-f86617e94715",
   "metadata": {},
   "source": [
    "# LLM-based qualitative and quantitative analysis of social media comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bdc028c-0ead-479b-a8f6-03ce4c945389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import os\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "\n",
    "# libraries for getting data from tiktok and youtube\n",
    "from TikTokApi import TikTokApi # https://github.com/davidteather/TikTok-Api\n",
    "from googleapiclient.discovery import build\n",
    "import asyncio\n",
    "\n",
    "# libraries for topic analysis\n",
    "from bertopic import BERTopic\n",
    "from deep_translator import GoogleTranslator\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "import plotly.io as pio\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "import langdetect\n",
    "import regex\n",
    "\n",
    "# LLM support for analysing the comments\n",
    "from mlx_lm import load, generate\n",
    "from transformers import pipeline\n",
    "from IPython.display import display\n",
    "import platform\n",
    "\n",
    "pio.renderers.default='iframe'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05697ff7-231a-4398-9c5c-0c77442b5671",
   "metadata": {},
   "source": [
    "# Get Data from TikTok and YouTube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f713fa4-db98-4461-8a79-33c3cdbe3761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the number of videos, comments and the hashtag\n",
    "hashtag = 'menstruation'\n",
    "num_videos = 100\n",
    "num_comments = 100\n",
    "date_threshold = 2023 # for now only year supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "724b2e00-9b99-456b-bfa6-9fd7f6e874f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access tokens\n",
    "ms_token_tiktok = ''\n",
    "my_api_key_youtube = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4be2174-c518-400d-8b2b-46170deeb55e",
   "metadata": {},
   "source": [
    "## TikTok"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f72ab9-331c-4b9c-99fa-b4d092d72439",
   "metadata": {},
   "source": [
    "### Get videos to hashtags and comments to video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4001979f-34c8-492b-9329-562c606320aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_hashtag_videos(api, hashtag, num_videos):\n",
    "    # await api.create_sessions(ms_tokens=[ms_token], num_sessions=1, sleep_after=3)\n",
    "    await api.create_sessions(headless=False, ms_tokens=[ms_token_tiktok], num_sessions=1, sleep_after=3)\n",
    "    tag = api.hashtag(name=hashtag)\n",
    "    count = 0\n",
    "    results = []\n",
    "    dates = []\n",
    "    async for video in tag.videos(count=num_videos*1000): # this count parameter is not quite working\n",
    "        date = video.as_dict['createTime']\n",
    "        date_converted = datetime.utcfromtimestamp(int(date)).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        if count < num_videos and int(date_converted[0:4]) >= date_threshold:\n",
    "            results.append(video.as_dict)\n",
    "            dates.append(date)\n",
    "            count += 1\n",
    "    return results, dates\n",
    "    \n",
    "async def get_comments(api, ms_token_tiktok, video_id, num_comments):\n",
    "    await api.create_sessions(headless=False, ms_tokens=[ms_token_tiktok], num_sessions=1, sleep_after=3)\n",
    "    video = api.video(id=video_id)\n",
    "    count = 0\n",
    "    comments = []\n",
    "    async for comment in video.comments(count=num_comments): # this count parameter is not quite working\n",
    "        if count < num_comments:\n",
    "            comments.append(comment.as_dict)\n",
    "            count += 1\n",
    "    return comments\n",
    "\n",
    "async def tiktok_get_comments_to_hashtag(api, hashtag, num_videos, num_comments):\n",
    "    \n",
    "    await asyncio.sleep(2)\n",
    "    results, dates = await get_hashtag_videos(api, hashtag, num_videos)\n",
    "    comments_all = []\n",
    "    for video in results:\n",
    "        if 0 == 0:\n",
    "            if 'id' in video:\n",
    "                comments = await get_comments(api, ms_token_tiktok, video['id'], num_comments)\n",
    "                comments_all.append(comments)\n",
    "    await api.close_sessions()\n",
    "    \n",
    "    return results, comments_all, dates\n",
    "\n",
    "def parse_tiktok_output(results, comments_all):\n",
    "    count = 0\n",
    "    videos = []\n",
    "    for result in results:\n",
    "        if 0 == 0:\n",
    "            if 'contents' in result:\n",
    "                # print(result['contents'])\n",
    "                comments = []\n",
    "                for item in result['contents']:\n",
    "                    desc = item['desc']\n",
    "                for comment in comments_all[count]:\n",
    "                    comments.append([comment['comment_language'],comment['text'].strip()])\n",
    "                videos.append({'id': count, 'stats':result['stats'], 'desc':desc, 'comments': comments})\n",
    "                count += 1\n",
    "    return videos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8ab282-5e89-471d-b283-62dd3094a65f",
   "metadata": {},
   "source": [
    "### Get comments from TikTok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc23e7f8-9e1b-4221-8272-a160876dd9a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "api = TikTokApi()\n",
    "results_tiktok, comments_all_tiktok, dates_tt = await tiktok_get_comments_to_hashtag(api, hashtag, num_videos, num_comments)\n",
    "\n",
    "videos_tiktok = parse_tiktok_output(results_tiktok, comments_all_tiktok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ac16db-b945-4a07-94f4-2f023ae12470",
   "metadata": {},
   "source": [
    "## YouTube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "569eae08-0867-49a8-821e-1ad68142d14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def access_video_yt(video_id, max_results):\n",
    "    \n",
    "    youtube = build('youtube', 'v3', developerKey=my_api_key_youtube)\n",
    "    request = youtube.videos().list(part='snippet,statistics', id=video_id)\n",
    "    # https://developers.google.com/youtube/v3/docs/commentThreads/list\n",
    "    comments_request = youtube.commentThreads().list(part='id,snippet,replies', videoId=video_id, maxResults=max_results, order='relevance')\n",
    "    \n",
    "    stats = request.execute()\n",
    "    try:\n",
    "        comments = comments_request.execute()\n",
    "    except:\n",
    "        comments = []\n",
    "        print(f'Video with ID {video_id} cannot be scraped for comments.')\n",
    "    \n",
    "    return stats, comments\n",
    "\n",
    "def extract_infos_yt(details):\n",
    "    \n",
    "    title = details['items'][0]['snippet']['title']\n",
    "    view_count = details['items'][0]['statistics']['viewCount']\n",
    "\n",
    "    return title, view_count\n",
    "\n",
    "def search_videos_yt(query, max_results):\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    youtube = build('youtube', 'v3', developerKey=my_api_key_youtube)\n",
    "    # https://developers.google.com/youtube/v3/docs/search/list\n",
    "    request = youtube.search().list(part='id', type='video', videoDuration='short', q=query, maxResults=max_results, order='relevance', publishedAfter=str(date_threshold)+'-01-01T00:00:00Z')\n",
    "    response = request.execute()\n",
    "    \n",
    "    for video in response['items']:\n",
    "        results.append(video['id']['videoId'])\n",
    "\n",
    "    while len(results) < max_results:\n",
    "        request = youtube.search().list(part='id', type='video', pageToken=response['nextPageToken'], videoDuration='short', q=query, maxResults=max_results, order='relevance', publishedAfter=str(date_threshold)+'-01-01T00:00:00Z')\n",
    "        response = request.execute()\n",
    "        for video in response['items']:\n",
    "            results.append(video['id']['videoId'])\n",
    "    \n",
    "    return results, response\n",
    "\n",
    "def extract_comments_yt(comments):\n",
    "    \n",
    "    comment_list = []\n",
    "\n",
    "    if len(comments) > 0:\n",
    "        for item in comments['items']:\n",
    "            comment_list.append(item['snippet']['topLevelComment']['snippet']['textDisplay'].strip())\n",
    "\n",
    "    return comment_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f53a649-15a0-477c-9ec8-71895ee86038",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_yt, response = search_videos_yt(hashtag, num_videos)\n",
    "\n",
    "videos_yt = []\n",
    "count = 0\n",
    "# save results_yt for video IDs\n",
    "for result in results_yt:\n",
    "    stats, comments = access_video_yt(result, num_comments)\n",
    "    title, view_count = extract_infos_yt(stats)\n",
    "    comments_list = extract_comments_yt(comments)\n",
    "    videos_yt.append({'id': count, 'stats':stats, 'desc':title, 'comments': comments_list})\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b428fb-23a3-465a-b3b6-3c9695bb4ca0",
   "metadata": {},
   "source": [
    "## Translate the comments\n",
    "Using https://pypi.org/project/deep-translator/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f8877d-b43f-4b3b-a43f-56d4a77d7632",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comments_tt = []\n",
    "comments_tt_per_video = []\n",
    "list_per_video_tt = []\n",
    "counter = 0\n",
    "for video in videos_tiktok:\n",
    "    # translate comment and analyse it\n",
    "    concat = ''\n",
    "    per_video = []\n",
    "    for comment in video['comments']:\n",
    "        comm = comment[1].strip()\n",
    "        if comm is None:\n",
    "            continue\n",
    "        if comment[0] == 'en':\n",
    "            comments_tt.append(comm)\n",
    "            concat += comm\n",
    "            concat += ';'\n",
    "            per_video.append(comm)\n",
    "        elif comment[0] == 'un':\n",
    "            print('un: ', comm)\n",
    "            continue\n",
    "        else:\n",
    "            # Use Google translator to tranlate everything that is not in English into English\n",
    "            translated = GoogleTranslator(source='auto', target='en').translate(comm)\n",
    "            translated = translated\n",
    "            if translated is not None:\n",
    "                comments_tt.append(translated)\n",
    "                concat += translated\n",
    "                concat += ';'\n",
    "                per_video.append(translated)\n",
    "    comments_tt_per_video.append(concat)\n",
    "    list_per_video_tt.append(per_video)\n",
    "\n",
    "\n",
    "comments_yt = []\n",
    "comments_yt_per_video = []\n",
    "list_per_video_yt = []\n",
    "print('youtube')\n",
    "counter = 0\n",
    "for video in videos_yt:\n",
    "    # translate comment and analyse it\n",
    "    concat = ''\n",
    "    per_video = []\n",
    "    for comment in video['comments']:\n",
    "        comment = comment.strip()\n",
    "        if comment is None:\n",
    "            continue\n",
    "        # Use Google translator to tranlate everything that is not in English into English\n",
    "        try:\n",
    "            language = langdetect.detect(comment)\n",
    "        except:\n",
    "            print(comment)\n",
    "            language = \"error\"\n",
    "        if language == 'en':\n",
    "            translated = comment\n",
    "        elif language == 'error':\n",
    "            continue\n",
    "        else:\n",
    "            try:\n",
    "                translated = GoogleTranslator(source='auto', target='en').translate(comment)\n",
    "            except:\n",
    "                print(comment)\n",
    "                translated = ''\n",
    "        if translated is not None:\n",
    "            concat += translated\n",
    "            concat += ';'\n",
    "            per_video.append(translated)\n",
    "            comments_yt.append(translated)\n",
    "    comments_yt_per_video.append(concat)\n",
    "    list_per_video_yt.append(per_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9d3d5f-f428-47f6-85b2-ac5359b326a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_yt = [x for x in comments_yt if x is not None]\n",
    "comments_tt = [x for x in comments_tt if x is not None]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d8015d-50a4-400a-88b5-2643363872e8",
   "metadata": {},
   "source": [
    "# Analyse the comments\n",
    "## BERTopic together with Llama Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1d4992-bd42-4c84-b720-fd42a1219193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise Llama Chatbot\n",
    "\n",
    "model, tokenizer = load(\"mlx-community/Meta-Llama-3-8B-Instruct-4bit\")\n",
    "\n",
    "SYSTEM_MSG = \"You are a helpful chatbot assistant that cares a lot about menstruation topics.\"\n",
    "\n",
    "def generateFromPrompt(promptStr,maxTokens=100):\n",
    "\n",
    "    messages = [ {\"role\": \"system\", \"content\": SYSTEM_MSG},\n",
    "              {\"role\": \"user\", \"content\": promptStr}, ]\n",
    "    input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "    prompt = tokenizer.decode(input_ids)\n",
    "    response = generate(model, tokenizer, prompt=prompt,max_tokens=maxTokens)\n",
    "\n",
    "    return(response)\n",
    "\n",
    "\n",
    "response = generateFromPrompt(\"Please introduce yourself\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "fdfa0580-97b1-45f6-82ce-ee138c5797df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define BERTopic model\n",
    "custom_stopwords = ['br', 'https', 'fp', 'ft', 'bro', 'lol', 'wsh', 'fr', 'rn', '39', 'href', 'youtube', 'href', 'www', 'com', 'quot']\n",
    "\n",
    "def run_BERTopic(comments):\n",
    "    # remove stop words\n",
    "    all_stopwords = list(ENGLISH_STOP_WORDS.union(custom_stopwords))  \n",
    "    vectorizer_model = CountVectorizer(\n",
    "        stop_words=all_stopwords,\n",
    "        min_df=5 \n",
    "    )\n",
    "    embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    umap_model = UMAP(n_neighbors=10, n_components=10, metric='cosine')\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size=20, min_samples=10, metric='euclidean') #cluster_selection_epsilon=0.5,\n",
    "\n",
    "    # Initialize BERTopic with more words per topic\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=embedding_model,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        top_n_words=15,  # Increase the number of words per topic\n",
    "        verbose=True\n",
    "    )\n",
    "    # Fit the model\n",
    "    topics, probabilities = topic_model.fit_transform(comments)\n",
    "    \n",
    "    # View the number of topics identified\n",
    "    print(\"Number of topics identified:\", len(topic_model.get_topic_info()) - 1)  # Exclude -1 for outliers\n",
    "\n",
    "    # Get topic information\n",
    "    topics_info = topic_model.get_topic_info()\n",
    "    \n",
    "    # Print each topic with only the words (no scores)\n",
    "    for topic_num in topics_info.Topic:\n",
    "        if topic_num != -1:  # Exclude the outlier topic\n",
    "            topic_words = [word for word, _ in topic_model.get_topic(topic_num)]  # Extract only words\n",
    "            print(f\"Topic {topic_num}: {', '.join(topic_words)}\")\n",
    "\n",
    "    return topic_model, topics_info\n",
    "\n",
    "def generate_topic_names(topic_model, topics_info):\n",
    "    # Generate topic names\n",
    "    topic_names = []\n",
    "    topic_nums = []\n",
    "    topic_wordlists = []\n",
    "    \n",
    "    for topic_num in topics_info.Topic:\n",
    "        if topic_num != -1:  # Exclude the outlier topic\n",
    "            topic_words = [word for word, _ in topic_model.get_topic(topic_num)]  # Extract only words\n",
    "            topic_nums.append(topic_num)\n",
    "            topic_names.append(generateFromPrompt(f\"The main topic is {hashtag}. Please give a not too general but conscience subtheme that unifies these words: {topic_words}. Only state the topic:\"))            \n",
    "            topic_wordlists.append(\",\".join(topic_words))\n",
    "    \n",
    "    df_topics = pd.DataFrame({\"Topic\":topic_nums,\"Name\":topic_names,\"Words\":topic_wordlists})\n",
    "    return df_topics\n",
    "\n",
    "def count_occurance(df_topics, topic_model, comments):\n",
    "    \n",
    "    topic_distr, _ = topic_model.approximate_distribution(comments)\n",
    "\n",
    "    # Get a binary matrix of documents per topic\n",
    "    df = pd.DataFrame(comments)\n",
    "    binary_df = df.iloc[:, : 9]\n",
    "    df_topics_merged = []\n",
    "    \n",
    "    THRESHOLD = 0.2 #Â Probability threshold to 'count' a topic as 'mentioned' in a document\n",
    "    \n",
    "    for topic in range(topic_distr.shape[1]): \n",
    "        docs = topic_distr[:,topic] > THRESHOLD\n",
    "        docs = [1 if d else 0 for d in docs]\n",
    "        df_topics['Name'][topic] = df_topics['Name'][topic].replace('\"','')\n",
    "        topic_name = df_topics['Name'][topic].replace('\"','')\n",
    "        if topic_name in binary_df.columns:\n",
    "            print(topic_name)\n",
    "            existing = binary_df.loc[:,topic_name].tolist()\n",
    "            merged = [or_fct(x,y) for x,y in zip(existing, docs)]\n",
    "            binary_df[topic_name] = merged\n",
    "        else:\n",
    "            binary_df[topic_name] = docs\n",
    "            df_topics_merged.append([topic_name, df_topics['Words'][topic]])\n",
    "\n",
    "    df_topics_merged = pd.DataFrame(df_topics_merged)\n",
    "    # count how often topic occurs in comments\n",
    "    column_counts = binary_df.iloc[:, 1 : ].sum(axis=0)\n",
    "    column_counts_format = []\n",
    "    for count in column_counts:\n",
    "        column_counts_format.append(count)\n",
    "    df_topics_merged['counter'] = column_counts_format\n",
    "\n",
    "    return topic_distr, df_topics_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5639988a-b72f-450e-bced-2e7c93ffab80",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model_yt, topics_info_yt = run_BERTopic(comments_yt)\n",
    "topic_model_tt, topics_info_tt = run_BERTopic(comments_tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "cc0eb406-36cd-431f-8cd4-d0ea888cff81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topics_yt = generate_topic_names(topic_model_yt, topics_info_yt)\n",
    "df_topics_tt = generate_topic_names(topic_model_tt, topics_info_tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abb1b79-40aa-4e95-8973-289cb16279f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging of same topics and counting of their occurences in the comments\n",
    "topic_distr_yt, df_topics_yt_merged = count_occurance(df_topics_yt, topic_model_yt, comments_yt)\n",
    "topic_distr_tt, df_topics_tt_merged = count_occurance(df_topics_tt, topic_model_tt, comments_tt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d2254f-bb9b-4bf4-8923-2eea0f1a80ab",
   "metadata": {},
   "source": [
    "# Llama Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "35ecdbd9-aec9-460a-b207-fbfae735cebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_yt_con = '|'.join(comments_yt)\n",
    "comment_tt_con = '|'.join(comments_tt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f73d56-8c1d-4697-99f2-6edface00d4d",
   "metadata": {},
   "source": [
    "## Identify Needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c95fedd-1ec2-408c-8b57-8c4d2d430397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that you might need further interations on split-up text if the text is too long\n",
    "needs_yt = ''\n",
    "for video in comments_yt_per_video:\n",
    "    if len(video) < 100:\n",
    "        continue\n",
    "    response = generateFromPrompt(f\"Please identify some needs of people interacting with {hashtag} videos in the following list of comments : ' \" + video + \"'\",maxTokens=200)\n",
    "    needs_yt += response\n",
    "    needs_yt += ';NEXT;'\n",
    "\n",
    "response = generateFromPrompt(\"Please summarise the summaries in the following text and identify 5 needs :' \" + needs_yt + \"'\",maxTokens=500)\n",
    "\n",
    "print(\"Needs for YouTube\\n \"+response+\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f0ddaf-4c99-42e3-bf65-e2df2c4f8b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that you might need further interations on split-up text if the text is too long\n",
    "needs_tt = ''\n",
    "for video in comments_tt_per_video:\n",
    "    if len(video) < 100:\n",
    "        continue\n",
    "    response = generateFromPrompt(f\"Please identify some needs of people interacting with {hashtag} videos in the following list of comments: ' \" + video + \"'\",maxTokens=200)\n",
    "    needs_tt += response\n",
    "    needs_tt += ';NEXT;'\n",
    "\n",
    "response = generateFromPrompt(\"Please summarise the summaries in the following text and identify 5 needs : ' \" + needs_tt + \"'\",maxTokens=500)\n",
    "\n",
    "print(\"Needs for TikTok\\n \"+response+\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea424b9-8db4-4f91-bec8-7ec494dd06e3",
   "metadata": {},
   "source": [
    "## Identify Experiences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1be48d8-5cdb-4ed8-ba31-8a63f201362d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that you might need further interations on split-up text if the text is too long\n",
    "exp_yt = ''\n",
    "for video in comments_yt_per_video:\n",
    "    if len(video) < 100:\n",
    "        continue\n",
    "    response = generateFromPrompt(f\"Please identify some experiences people interacting with {hashtag} videos have in the following list of comments: ' \" + video + \"'\",maxTokens=200)\n",
    "    exp_yt += response\n",
    "    exp_yt += ';NEXT;'\n",
    "\n",
    "response = generateFromPrompt(\"Please summarise the following text and identify 5 experiences: ' \" + exp_yt + \"'\",maxTokens=500)\n",
    "\n",
    "print(\"Experiences for YouTube\\n \"+response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582fa801-bf9a-4e49-a10b-6e409a5280dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# note that you might need further interations on split-up text if the text is too long\n",
    "exp_tt = ''\n",
    "for video in comments_tt_per_video:\n",
    "    if len(video) < 100:\n",
    "        continue\n",
    "    response = generateFromPrompt(f\"Please identify some experiences people interacting with {hashtag} videos have in the following list of comments: ' \" + video + \"'\",maxTokens=200)\n",
    "    exp_tt += response\n",
    "    exp_tt += ';NEXT;'\n",
    "\n",
    "response = generateFromPrompt(\"Please summarise the summaries in the following text and identify 5 experiences : ' \" + exp_tt + \"'\",maxTokens=500)\n",
    "\n",
    "print(\"Experiences for TikTok\\n \"+response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e237d969-5317-4f3b-aff1-f9129805b7ed",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e1562c-2694-4310-8d34-a7203e3c2810",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(layout=\"constrained\", figsize=(12, 3))\n",
    "gs = GridSpec(1, 2, figure=fig)\n",
    "ax4 = fig.add_subplot(gs[1])\n",
    "ax5 = fig.add_subplot(gs[0])\n",
    "\n",
    "# 4: show topics and their count as identified by BERTopics + Llama for YouTube\n",
    "df_topics_yt_merged_sorted = df_topics_yt_merged.sort_values(by=['counter'], ascending=False)\n",
    "x_axis_yt = df_topics_yt_merged_sorted['counter'].to_list()\n",
    "x_axis_yt_pc = [round(counter/len(comments_yt),2) for counter in x_axis_yt]\n",
    "y_axis_yt_pre = df_topics_yt_merged_sorted[0].to_list()\n",
    "y_axis_yt = []\n",
    "for y in y_axis_yt_pre:\n",
    "    if len(y) > 20:\n",
    "        y_split = y.split(' ')\n",
    "        y_split[int(len(y_split)/2) - 1] += '\\n'\n",
    "        y_axis_yt.append(' '.join(y_split))\n",
    "    else:\n",
    "        y_axis_yt.append(y)\n",
    "hbars = ax4.barh(y_axis_yt[:5], x_axis_yt_pc[:5], align='center')\n",
    "ax4.set_yticks(y_axis_yt[:5])\n",
    "ax4.bar_label(hbars, labels=[ f'{n} \\n{round(100*n_pc,2)}%' for n,n_pc in zip(x_axis_yt[:5], x_axis_yt_pc[:5])],\n",
    "             padding=-28, color='black', fontsize=8, fontweight='bold')\n",
    "\n",
    "ax4.set_title(\"Relative occurrence per topic\\n in comments for YouTube\", fontweight='bold')\n",
    "\n",
    "\n",
    "# 5: show topics and their count as identified by BERTopics + Llama for TikTok\n",
    "df_topics_tt_merged_sorted = df_topics_tt_merged.sort_values(by=['counter'], ascending=False)\n",
    "x_axis_tt = df_topics_tt_merged_sorted['counter'].to_list()\n",
    "x_axis_tt_pc = [round(counter/len(comments_tt),2) for counter in x_axis_tt]\n",
    "y_axis_tt_pre = df_topics_tt_merged_sorted[0].to_list()\n",
    "y_axis_tt = []\n",
    "for y in y_axis_tt_pre:\n",
    "    if len(y) > 20:\n",
    "        y_split = y.split(' ')\n",
    "        y_split[int(len(y_split)/2) - 1] += '\\n'\n",
    "        y_axis_tt.append(' '.join(y_split))\n",
    "    else:\n",
    "        y_axis_tt.append(y)\n",
    "\n",
    "hbars = ax5.barh(y_axis_tt[:5], x_axis_tt_pc[:5], align='center')\n",
    "ax5.set_yticks(y_axis_tt[:5])\n",
    "ax5.invert_yaxis()  # labels read top-to-bottom\n",
    "ax5.bar_label(hbars, labels=[ f'{n} \\n{round(100*n_pc,2)}%' for n,n_pc in zip(x_axis_tt[:5], x_axis_tt_pc[:5])],\n",
    "             padding=-28, color='black', fontsize=8, fontweight='bold')\n",
    "\n",
    "ax5.set_title(\"Relative occurrence per topic\\n in comments for TikTok\", fontweight='bold')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09c56f0-0535-417c-9d42-bfd872c2345c",
   "metadata": {},
   "source": [
    "## Save video IDs and date and input parameter of analysis\n",
    "- IDs of TikTok videos\n",
    "- IDs of YouTube videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a41d3ac4-1dcf-4324-94fa-f47642e99022",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../social_analysis_videos.txt\", \"a\")\n",
    "\n",
    "f.write('Analysis conducted on ' + str(datetime.now())  + '.\\n')\n",
    "f.write(f'Using the hashtag/ searchterm {hashtag}, searching for maximum {num_videos} videos with maximum {num_comments} comments each.\\n')\n",
    "\n",
    "f.write('YouTube\\n')\n",
    "for result in results_yt:\n",
    "    f.write(result + '\\n')\n",
    "f.write('TikTok\\n')\n",
    "for result in results_tiktok:\n",
    "    f.write(result['id'] + '\\n')\n",
    "\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
